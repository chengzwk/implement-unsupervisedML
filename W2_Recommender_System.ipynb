{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ca277d-5e8b-4c75-b017-6363e0c2b233",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Recommender System\n",
    "In this notebook, we'll implement collaborative filtering to build a recommender system for movies (and use it on two sample datasets from the course graded lab).  \n",
    "The code here are based on my own implementations in the graded lab, organized and rewritten to be more succinct and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f966fd-3d88-4778-83dd-be049126756f",
   "metadata": {},
   "source": [
    "### Tensors and operations in tensorflow\n",
    "See this [link](https://www.tensorflow.org/tutorials/customization/basics) for tensors and their operations, as well as the conversion between tensors and numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b244334-6b75-4a70-9349-2a69961c4599",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67bde927-25eb-4513-98a5-058dedcaefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87bf1d-6338-46c1-abf4-3b210177e520",
   "metadata": {},
   "source": [
    "## Collaborative Filtering Algorithm\n",
    "\n",
    "We'll be predicting the movie ratings as follows: for user $j$, predict rating for movie $i$ as:\n",
    "\n",
    "$$\n",
    "Y(i,j) = R(i,j) * (\\vec{w}^{(j)} \\cdot \\vec{x}^{(i)} + b^{(j)})\n",
    "$$\n",
    "\n",
    "where $R(i,j) = 1$ if user $j$ has rated movie $i$, and $R(i,j) = 0$ if not.\n",
    "\n",
    "We'll use gradient descent with the following cost function to learn the parameters $\\mathbf{X}$, $\\mathbf{W}$, and $\\mathbf{b}$ collaboratively.\n",
    "\n",
    "$$J({\\mathbf{x}^{(0)},...,\\mathbf{x}^{(n_m-1)},\\mathbf{w}^{(0)},b^{(0)},...,\\mathbf{w}^{(n_u-1)},b^{(n_u-1)}})= \\left[ \\frac{1}{2}\\sum_{(i,j):r(i,j)=1}(\\mathbf{w}^{(j)} \\cdot \\mathbf{x}^{(i)} + b^{(j)} - y^{(i,j)})^2 \\right]\n",
    "+ \\underbrace{\\left[\n",
    "\\frac{\\lambda}{2}\n",
    "\\sum_{j=0}^{n_u-1}\\sum_{k=0}^{n-1}(\\mathbf{w}^{(j)}_k)^2\n",
    "+ \\frac{\\lambda}{2}\\sum_{i=0}^{n_m-1}\\sum_{k=0}^{n-1}(\\mathbf{x}_k^{(i)})^2\n",
    "\\right]}_{regularization}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ec891-eb1c-4854-8a6d-be8262d4058e",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Cost function\n",
    "We'll implement the cost function with the following function:\n",
    "- `cofi_cost_func`: compute cost for collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97aedec5-0d4d-45b8-b36a-bb08c7be67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    \"\"\"\n",
    "    Returns the cost for the collaborative filtering\n",
    "    Vectorized for speed. Uses tensorflow operations to be compatible with custom training loop.\n",
    "    Args:\n",
    "      X (ndarray (num_movies,num_features)): matrix of item features\n",
    "      W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "      b (ndarray (1, num_users)            : vector of user parameters\n",
    "      Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "      R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "      lambda_ (float): regularization parameter\n",
    "    Returns:\n",
    "      J (float) : Cost\n",
    "    \"\"\"\n",
    "    J = tf.math.reduce_sum((R * (tf.linalg.matmul(X, tf.transpose(W)) + b - Y))**2)\n",
    "    reg = lambda_ * (tf.math.reduce_sum(W**2) + tf.math.reduce_sum(X**2))\n",
    "    J = (J + reg) / 2\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da28c38-b25b-4760-b992-f552102045a8",
   "metadata": {},
   "source": [
    "### Model training\n",
    "We'll train the model with a custom training loop in tensorflow with the Adam optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edcc18a5-8f43-4b88-b1e0-cce1ab5624a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Parameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m n_m, n_u \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# n_m: number of movies, n_u: number of users\u001b[39;00m\n\u001b[1;32m      3\u001b[0m n_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# number of features for each movie\u001b[39;00m\n\u001b[1;32m      4\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-1\u001b[39m  \u001b[38;5;66;03m# learning rate\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_m, n_u = Y.shape  # n_m: number of movies, n_u: number of users\n",
    "n_f = 100  # number of features for each movie\n",
    "alpha = 1e-1  # learning rate\n",
    "\n",
    "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "# Randomly initalized parameters W, b, X\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W = tf.Variable(tf.random.normal([n_u, n_f], dtype=tf.float64), name='W')\n",
    "X = tf.Variable(tf.random.normal([n_m, n_f], dtype=tf.float64), name='X')\n",
    "b = tf.Variable(tf.random.normal([1, n_u], dtype=tf.float64), name='b')\n",
    "\n",
    "# Instatntiate on optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate = alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41909fd-6ef2-4faa-b79a-440a94ca18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 200\n",
    "lambda_ = 1.\n",
    "\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlowâ€™s GradientTape\n",
    "    # to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = cofi_cost_func(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
